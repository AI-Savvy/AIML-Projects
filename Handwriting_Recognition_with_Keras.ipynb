{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Su7WoqFhwrwA"
      },
      "source": [
        "# Handwriting Recognition with Keras (CNN)\n",
        "\n",
        "Keras on the MNIST data set."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Libraries"
      ],
      "metadata": {
        "id": "iyplfVkgy5bv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "XnsIfyo4wrwC"
      },
      "outputs": [],
      "source": [
        "import tensorflow\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten\n",
        "from tensorflow.keras.optimizers import RMSprop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1c6Db9MwrwC"
      },
      "source": [
        "## Load MNIST Raw data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "9zqsTK6zwrwD"
      },
      "outputs": [],
      "source": [
        "(mnist_train_images, mnist_train_labels), (mnist_test_images, mnist_test_labels) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 2D image custom pixel shaping\n",
        "- Single color channel(gray scale = 1)\n",
        "- shape (1x28x28, 28x28x1)"
      ],
      "metadata": {
        "id": "HEK5asxfzEjc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "SX4c7hABwrwD"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import backend as K\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    train_images = mnist_train_images.reshape(mnist_train_images.shape[0], 1, 28, 28)\n",
        "    test_images = mnist_test_images.reshape(mnist_test_images.shape[0], 1, 28, 28)\n",
        "    input_shape = (1, 28, 28)\n",
        "else:\n",
        "    train_images = mnist_train_images.reshape(mnist_train_images.shape[0], 28, 28, 1)\n",
        "    test_images = mnist_test_images.reshape(mnist_test_images.shape[0], 28, 28, 1)\n",
        "    input_shape = (28, 28, 1)\n",
        "    \n",
        "train_images = train_images.astype('float32')\n",
        "test_images = test_images.astype('float32')\n",
        "train_images /= 255\n",
        "test_images /= 255"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CDyKVx5wrwD"
      },
      "source": [
        "## Tensorflow version of OneHotEncoding for categorizing lables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "XsCrlm00wrwD"
      },
      "outputs": [],
      "source": [
        "train_labels = tensorflow.keras.utils.to_categorical(mnist_train_labels, 10)\n",
        "test_labels = tensorflow.keras.utils.to_categorical(mnist_test_labels, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "vIXM2heowrwE",
        "outputId": "8c0d6696-263d-4b56-d9ce-b49c46d82147"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "# Plotting the image\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def display_sample(num):\n",
        "    #Print the one-hot array of this sample's label \n",
        "    print(train_labels[num])  \n",
        "\n",
        "    #Print the label converted back to a number\n",
        "    label = train_labels[num].argmax(axis=0)\n",
        "\n",
        "    #Reshape the 768 values to a 28x28 image\n",
        "    image = train_images[num].reshape([28,28])\n",
        "\n",
        "    # Naming the plot points and labels\n",
        "    plt.title('Sample: %d  Label: %d' % (num, label))\n",
        "    plt.imshow(image, cmap=plt.get_cmap('gray_r'))\n",
        "    plt.show()\n",
        "    \n",
        "display_sample(7390)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing CNN\n",
        "\n"
      ],
      "metadata": {
        "id": "3orpxe5w0ZBF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q97ZkifXwrwF",
        "outputId": "d555c8e7-5234-4d6c-decf-86dd7373ba52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_4 (Conv2D)           (None, 26, 26, 32)        320       \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 24, 24, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 12, 12, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 12, 12, 64)        0         \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 9216)              0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 128)               1179776   \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,199,882\n",
            "Trainable params: 1,199,882\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "\n",
        "# 64 3x3 kernels\n",
        "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "\n",
        "# Reduce by taking the max of each 2x2 block\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "# Dropout to avoid overfitting\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# Flatten the results to one dimension for passing into our final layer\n",
        "model.add(Flatten())\n",
        "\n",
        "# A hidden layer to learn with\n",
        "model.add(Dense(128, activation='relu'))\n",
        "\n",
        "# Another dropout\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Final categorization from 0-9 with softmax\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "KEP2EozywrwG"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy']) # loss function - categorical corss entropy along with adam optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xVuRl2NWwrwG",
        "outputId": "1f48b294-5278-4720-b1ec-951f3ac09033"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "938/938 - 5s - loss: 0.2055 - accuracy: 0.9386 - val_loss: 0.0495 - val_accuracy: 0.9845 - 5s/epoch - 6ms/step\n",
            "Epoch 2/25\n",
            "938/938 - 5s - loss: 0.0833 - accuracy: 0.9743 - val_loss: 0.0413 - val_accuracy: 0.9870 - 5s/epoch - 5ms/step\n",
            "Epoch 3/25\n",
            "938/938 - 5s - loss: 0.0631 - accuracy: 0.9808 - val_loss: 0.0372 - val_accuracy: 0.9888 - 5s/epoch - 5ms/step\n",
            "Epoch 4/25\n",
            "938/938 - 4s - loss: 0.0525 - accuracy: 0.9839 - val_loss: 0.0357 - val_accuracy: 0.9882 - 4s/epoch - 5ms/step\n",
            "Epoch 5/25\n",
            "938/938 - 4s - loss: 0.0425 - accuracy: 0.9866 - val_loss: 0.0310 - val_accuracy: 0.9912 - 4s/epoch - 5ms/step\n",
            "Epoch 6/25\n",
            "938/938 - 4s - loss: 0.0382 - accuracy: 0.9879 - val_loss: 0.0313 - val_accuracy: 0.9907 - 4s/epoch - 5ms/step\n",
            "Epoch 7/25\n",
            "938/938 - 5s - loss: 0.0334 - accuracy: 0.9894 - val_loss: 0.0312 - val_accuracy: 0.9904 - 5s/epoch - 5ms/step\n",
            "Epoch 8/25\n",
            "938/938 - 4s - loss: 0.0299 - accuracy: 0.9903 - val_loss: 0.0301 - val_accuracy: 0.9916 - 4s/epoch - 5ms/step\n",
            "Epoch 9/25\n",
            "938/938 - 4s - loss: 0.0268 - accuracy: 0.9913 - val_loss: 0.0318 - val_accuracy: 0.9913 - 4s/epoch - 5ms/step\n",
            "Epoch 10/25\n",
            "938/938 - 5s - loss: 0.0238 - accuracy: 0.9923 - val_loss: 0.0369 - val_accuracy: 0.9904 - 5s/epoch - 5ms/step\n",
            "Epoch 11/25\n",
            "938/938 - 5s - loss: 0.0233 - accuracy: 0.9923 - val_loss: 0.0265 - val_accuracy: 0.9928 - 5s/epoch - 5ms/step\n",
            "Epoch 12/25\n",
            "938/938 - 5s - loss: 0.0191 - accuracy: 0.9938 - val_loss: 0.0326 - val_accuracy: 0.9915 - 5s/epoch - 5ms/step\n",
            "Epoch 13/25\n",
            "938/938 - 5s - loss: 0.0189 - accuracy: 0.9937 - val_loss: 0.0287 - val_accuracy: 0.9921 - 5s/epoch - 5ms/step\n",
            "Epoch 14/25\n",
            "938/938 - 4s - loss: 0.0171 - accuracy: 0.9945 - val_loss: 0.0314 - val_accuracy: 0.9921 - 4s/epoch - 5ms/step\n",
            "Epoch 15/25\n",
            "938/938 - 4s - loss: 0.0175 - accuracy: 0.9943 - val_loss: 0.0332 - val_accuracy: 0.9930 - 4s/epoch - 5ms/step\n",
            "Epoch 16/25\n",
            "938/938 - 5s - loss: 0.0154 - accuracy: 0.9948 - val_loss: 0.0321 - val_accuracy: 0.9920 - 5s/epoch - 5ms/step\n",
            "Epoch 17/25\n",
            "938/938 - 4s - loss: 0.0148 - accuracy: 0.9952 - val_loss: 0.0300 - val_accuracy: 0.9926 - 4s/epoch - 5ms/step\n",
            "Epoch 18/25\n",
            "938/938 - 5s - loss: 0.0142 - accuracy: 0.9953 - val_loss: 0.0341 - val_accuracy: 0.9919 - 5s/epoch - 5ms/step\n",
            "Epoch 19/25\n",
            "938/938 - 5s - loss: 0.0142 - accuracy: 0.9951 - val_loss: 0.0357 - val_accuracy: 0.9930 - 5s/epoch - 5ms/step\n",
            "Epoch 20/25\n",
            "938/938 - 4s - loss: 0.0139 - accuracy: 0.9952 - val_loss: 0.0342 - val_accuracy: 0.9923 - 4s/epoch - 5ms/step\n",
            "Epoch 21/25\n",
            "938/938 - 5s - loss: 0.0126 - accuracy: 0.9958 - val_loss: 0.0343 - val_accuracy: 0.9923 - 5s/epoch - 5ms/step\n",
            "Epoch 22/25\n",
            "938/938 - 5s - loss: 0.0121 - accuracy: 0.9958 - val_loss: 0.0332 - val_accuracy: 0.9918 - 5s/epoch - 5ms/step\n",
            "Epoch 23/25\n",
            "938/938 - 4s - loss: 0.0116 - accuracy: 0.9964 - val_loss: 0.0363 - val_accuracy: 0.9916 - 4s/epoch - 5ms/step\n",
            "Epoch 24/25\n",
            "938/938 - 5s - loss: 0.0125 - accuracy: 0.9958 - val_loss: 0.0312 - val_accuracy: 0.9926 - 5s/epoch - 5ms/step\n",
            "Epoch 25/25\n",
            "938/938 - 5s - loss: 0.0115 - accuracy: 0.9961 - val_loss: 0.0387 - val_accuracy: 0.9923 - 5s/epoch - 5ms/step\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(train_images, train_labels,\n",
        "                    batch_size=64,\n",
        "                    epochs=25,\n",
        "                    verbose=2,\n",
        "                    validation_data=(test_images, test_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDL_4XxMwrwH",
        "outputId": "9983ecbf-0024-4528-c2a9-2e6a92ed9fa5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.027972299605607986\n",
            "Test accuracy: 0.9922000169754028\n"
          ]
        }
      ],
      "source": [
        "score = model.evaluate(test_images, test_labels, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}